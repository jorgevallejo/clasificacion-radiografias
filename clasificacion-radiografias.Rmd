---
title: "Clasificación de imagenes de radiografias de tórax entre normales y con derrame"
subtitle: "Machine Learning - PEC 1"
author: "Jorge Vallejo Ortega"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document: 
      toc: true
      toc_float: true
  pdf_document:
      toc: true
# Next code for knitting both types of documents automatically comes from https://stackoverflow.com/questions/39662365/knit-one-markdown-file-to-two-output-files/53280491#53280491
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_format = "all") })
bibliography: scholar.bib
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE)

```

```{r libraries, include=FALSE}
# Install packages
# Load packages
# ...

library(knitr)
```

# Índice

# Algoritmo k-NN

El algoritmo de los k vecinos más próximos (_k-nearest neighbours_) es un algoritmo de aprendizaje automático (_machine learning_) que se utiliza para clasificar observaciones, según si sus características las hacen más parecidas a uno u otro grupo ya establecidos.  

En una primera fase de 'entrenamiento', una colección de observaciones ya clasificadas se distribuyen en un espacio _n_-dimensional. Cada dimensión corresponde a una de las variables medidas en las observaciones. Las nuevas observaciones, las cuales queremos clasificar, se distribuyen a su vez en ese espacio _n_-dimensional; y se clasifican dentro de los grupos a los que pertenezcan aquellas otras observaciones, ya clasificadas, de las que más cerca se encuentren. El número de observaciones conocidas que tenemos en cuenta para clasificar las observaciones nuevas, es ése número _k_.


## Fortalezas y debilidades del algoritmo

```{r fortalezas debilidades, echo=FALSE, results='asis'}
fort_deb <- matrix(c("*Simple y efectivo",
              "* No hace asunciones acerca de la distribución subyacente de los datos",
              "* Fase de entrenamiento rápida",
              " ",
              "* No produce un modelo, limitando la habilidad para entender cómo las características se relacionan con la clasificación",
              "* Requiere seleccionar una _k_ adecuada",
              "* Fase de clasificación lenta",
              "* Se requiere procesado adicional para características nominales y datos incompletos"),
              ncol=2)
col_names <- c("Fortalezas", "Debilidades")
kable(fort_deb, col.names = col_names)
```


# Pre-procesado de datos

Las imágenes a clasificar deberán estar en una carpeta llamada "dataset", en el mismo directorio que el código que vamos a correr.
```{r set directories structure}
dir.create("./results")
```

```{r function image to vector}
library('OpenImageR')

image_tovector <- function(x){
# Create filepath
filepath <- paste("./dataset", sep='/',
                  paste(as.vector(x), collapse = '/'))
#print(filepath)
# Read image
image <- readImage(filepath)
# Convert to greyscale
image_grey <- rgb_2gray(image)
# Resize to 64x64
image_resized <- resizeImage(image_grey, 64, 64)
# Transform from matrix to vector
return(as.vector(image_resized))
}

## TODO ## Incluir en el informe ejemplos de imagenes antes y después del preprocesado (función imageShow()).

```

```{r extract directories and filenames}
directories <- list.dirs("./dataset", full.names = FALSE, recursive = FALSE)
filenames <- as.data.frame(
  sapply(directories,
                    function (x){return(
                      list.files(paste0("./dataset/", x)))}))

# From wide to long format
filenames <- reshape(filenames,
                      direction="long",
                      varying = c("effusion", "normal"),
                      v.names="Filename",
                      timevar="cat",
                      times = c("effusion", "normal")
                      )
row.names(filenames) <- NULL
filenames$id <- NULL
#filenames$Condition <- as.factor(filenames$Condition)
filenames$Filename <- as.character(filenames$Filename)
```

```{r create dataframe of vectorized images}
# Matrix of vectorized images
image_vectors <- apply(filenames, 1, image_tovector)
## TODO Keep the file names as col names

# This instructions to avoid re-calculating the object every time
load("image_vectors.RData")
save(image_vectors, file = "image_vectors.RData")

# Transpose to have images as observations
image_vectors <- as.data.frame(t(image_vectors))
```

```{r test images in rows, eval=FALSE}
## How to check if each row if really an x-ray image:
test_row <- function(x){
# Extract one row of the dataframe and store as a numeric vector
 test <- as.numeric(image_vectors[x,])
# Transform vector into matrix
 test <- matrix(test, ncol=64)
# Check if you obtain an x-ray image
 imageShow(test)
}
```

```{r final dataframe with filenames and diagnostic}
image_vectors <- cbind(filenames, image_vectors)
# Change cat variable to factors
image_vectors$cat <- as.factor(image_vectors$cat)
levels(image_vectors$cat) <- c("e", "n")
```

```{r write the dataframe into a csv file}
write.csv2(image_vectors,
           file="./results/RX_Torax_4097.csv",
           row.names = FALSE)
```

### Lectura de datos
```{r lectura del fichero csv}
image_vectors <- read.csv2("./results/RX_Torax_4097.csv")
```


### Estructura de los datos
```{r examine data structure}
observaciones <- nrow(image_vectors)
variables <- ncol(image_vectors)-2
 
```

El set de datos examinado está compuesto por `r observaciones` observaciones, de las cuales se han tomado `r variables` variables, y están divididas en `r length(directories)` clases (`r directories`).




La variable _id_ es un número identificador único para cada observación. Los identificadores únicos son contraproducentes en los métodos de aprendizaje automático, así que procederemos a eliminarlo.
```{r drop id variable}
wbcd$id <- NULL
```

La variable _diagnosis_ se usará como diana, ya que es lo que querremos predecir. Si exploramos la variable con la función table():
```{r}
table(wbcd$diagnosis)
```

Muchos algoritmos de clasificación requieren que la característica diana esté codificada como factor, y eso es lo que haremos a continuación, también aprovechando para utilizar unas etiquetas más informativas:
```{r recode diagnosis as factors}
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
                         labels = c("Benigno", "Maligno"))
```

Si examinamos las proporciones de cada diagnóstico:
```{r}
round(prop.table(table(wbcd$diagnosis))*100, digits = 1L)
```

```{r echo=FALSE}
# Para el informe dinámico una cosa que se puede hacer es comparar automáticamente los rangos de las variables y hacer la normalización, o no, según los órdenes de magnitud de diferencia.
```

```{r}
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
```

## 3- Transformación de datos numéricos: normalización

```{r define función de normalización}
normalize <- function(x){
  return ((x - min(x)) / (max(x) - min(x)))
}
```

```{r normalize all the numeric variables in the dataframe}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
```

```{r check if normalization worked}
summary(wbcd_n$area_mean)
```

## Preparación de los datos - crear los dataset de entrenamiento y de test
```{r}
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
```

```{r storing the dignosis labels}
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
```

### Step 3 - training a model on the data

For the k-NN algorithm, the training phase involves no model building; the process of training a lazy learner involves storing the input data in a structured format.

```{r classify the test data}
library('class')

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
                      cl = wbcd_train_labels, k=21)
```

### Step 4 - evaluating model performance

