---
title: "Clasificación de imagenes de radiografias de tórax entre normales y con derrame"
subtitle: "Machine Learning - PEC 1"
author: "Jorge Vallejo Ortega"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document: 
      toc: true
      toc_float: true
  pdf_document:
      toc: true
      #pandoc_args: -V toc-title="Sumario"
# Next code for knitting both types of documents automatically comes from https://stackoverflow.com/questions/39662365/knit-one-markdown-file-to-two-output-files/53280491#53280491
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_format = "all",
  output_dir = "results") })
# And:
# https://stackoverflow.com/a/46007686/10647267

---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = FALSE)

# This is a try:
knitr::opts_knit$set(stop_on_error = 2L)
# See ?evaluate::evaluate
# What I am trying to do is to make knitr stop
# when an error is found instead of running the
# complete script.
```

```{r delete results files, eval= FALSE}
# Run this chunk ONLY if you want to re-do
# all the report FROM ZERO.
# Remember that the .RData files are there to
# avoid unnecesarily redoing long data processing.

file.remove(c("clasificacion-radiografias.html",
              "clasificacion-radiografias.pdf",
              "./results/evaluated_k.RData",
              "./results/prob_pred.RData",
              "./results/RX_Torax_4097.csv"))


```


```{r libraries, include=FALSE}
# Install packages
# Load packages
# ...

library(knitr)
library('OpenImageR')
library('class')
library('gmodels')
library(ROCR)
```
\newpage
# Algoritmo k-NN

El algoritmo de los k vecinos más próximos (_k-nearest neighbours_) es un algoritmo de aprendizaje automático (_machine learning_) que se utiliza para clasificar observaciones, según si sus características las hacen más parecidas a uno u otro grupo ya establecidos.  

En una primera fase de 'entrenamiento', una colección de observaciones ya clasificadas se distribuyen en un espacio _n_-dimensional. Cada dimensión corresponde a una de las variables medidas en las observaciones. Las nuevas observaciones, las cuales queremos clasificar, se distribuyen a su vez en ese espacio _n_-dimensional; y se clasifican dentro de los grupos a los que pertenezcan aquellas otras observaciones, ya clasificadas, de las que más cerca se encuentren. El número de observaciones conocidas que tenemos en cuenta para clasificar las observaciones nuevas, es ése número _k_.


## Fortalezas y debilidades del algoritmo

```{r fortalezas debilidades, message=FALSE, results='asis'}
fort_deb <- matrix(c("*Simple y efectivo",
              "* No hace asunciones acerca de la distribución subyacente de los datos",
              "* Fase de entrenamiento rápida",
              " ",
              "* No produce un modelo, limitando la habilidad para entender cómo las características se relacionan con la clasificación",
              "* Requiere seleccionar una _k_ adecuada",
              "* Fase de clasificación lenta",
              "* Se requiere procesado adicional para características nominales y datos incompletos"),
              ncol=2)
col_names <- c("Fortalezas", "Debilidades")
kable(fort_deb, col.names = col_names)
```


# Pre-procesado de datos

```{r set directories structure}
dir.create("./results", showWarnings = FALSE)

# Las imágenes a clasificar deberán estar en una carpeta llamada "dataset",
# en el mismo directorio que el código que vamos a correr.
if (! dir.exists("./dataset")) {
  stop("El directorio ./dataset no existe")
}

# DONE Si datasets existe
#    si contiene datos
#        Continúa con el script
#    else: Aviso (La carpeta datasets no contiene datos)
# DONE Else: la carpeta datasets no existe
```

```{r function image to vector}

image_tovector <- function(x){
# Create filepath
filepath <- paste("./dataset", sep='/',
                  paste(as.vector(x), collapse = '/'))
#print(filepath)
# Read image
image <- readImage(filepath)
# Convert to greyscale
image_grey <- rgb_2gray(image)
# Resize to 64x64
image_resized <- resizeImage(image_grey, 64, 64)
# Transform from matrix to vector
return(as.vector(image_resized))
}

## TODO ## Incluir en el informe ejemplos de imagenes antes y después del preprocesado (función imageShow()).

```

```{r extract directories and filenames}
directories <- list.dirs("./dataset", full.names = FALSE, recursive = FALSE)
filenames <- as.data.frame(
  sapply(directories,
                    function (x){return(
                      list.files(paste0("./dataset/", x)))}))

# From wide to long format
filenames <- reshape(filenames,
                      direction="long",
                      varying = c("effusion", "normal"),
                      v.names="Filename",
                      timevar="cat",
                      times = c("effusion", "normal")
                      )
row.names(filenames) <- NULL
filenames$id <- NULL
#filenames$Condition <- as.factor(filenames$Condition)
filenames$Filename <- as.character(filenames$Filename)
```

```{r create dataframe of vectorized images}

#Check if the file already exists
if(file.exists("./results/image_vectors.RData")){
  load("./results/image_vectors.RData")
}else{
# Matrix of vectorized images
image_vectors <- apply(filenames, 1, image_tovector)
save(image_vectors, file = "./results/image_vectors.RData")
}
# This if-else block is to avoid re-calculating 
# the object every time I want to try the code.
# I have just have care to remove the .RData file
# if I want to re-calculate it.

## TODO Keep the file names as col names

# Transpose to have images as observations
image_vectors <- as.data.frame(t(image_vectors))
```

```{r test images in rows, eval=FALSE}
## How to check if each row is really an x-ray image:
test_row <- function(x){
# Extract one row of the dataframe and store as a numeric vector
 test <- as.numeric(image_vectors[x,])
# Transform vector into matrix
 test <- matrix(test, ncol=64)
# Check if you obtain an x-ray image
 imageShow(test)
}
```

```{r final dataframe with filenames and diagnostic}
image_vectors <- cbind(filenames, image_vectors)
# Change cat variable to factors
image_vectors$cat <- as.factor(image_vectors$cat)
levels(image_vectors$cat) <- c("e", "n")
```

```{r write the dataframe into a csv file}
if( ! file.exists("./results/RX_Torax_4097.csv")){
  write.csv2(image_vectors,
           file="./results/RX_Torax_4097.csv",
           row.names = FALSE)
}

# Structured in that way to avoid writing the file
# each time that I want to test the script,
# especially if I just want to test knitting
```

```{r lectura del fichero csv}
image_vectors <- read.csv2("./results/RX_Torax_4097.csv")
```

## Estructura de los datos
```{r examine data structure}
observaciones <- nrow(image_vectors)
variables <- ncol(image_vectors)-2
 
clases <- length(directories)
niveles <- levels(image_vectors$cat)
```

El set de datos examinado está compuesto por `r observaciones` observaciones.  
De cada observación se han tomado `r variables` variables.  
El conjunto de observaciones está dividido en `r clases` clases (`r directories[1:clases-1]` y `r directories[-1]`), codificadas como `r niveles[1:length(niveles)-1]` y `r niveles[-1]`.

La distribución de cada clase es la siguiente:

```{r}
kable(table(image_vectors$cat),
      col.names = c("Clase", "Frecuencia"),
      align = c('r','l'))
```

## Histogramas de las medias

```{r calcula la media de cada variable}
image_vectors_normal <- image_vectors[image_vectors$cat == 'n',]
image_vectors_effusion <- image_vectors[image_vectors$cat == 'e',]

media_normal <- apply(image_vectors_normal[,3:ncol(image_vectors_normal)], 2, mean)
media_effusion <- apply(image_vectors_effusion[,3:ncol(image_vectors_effusion)], 2, mean)
```

```{r calcula la desviación típica de cada variable}
sd_normal <- apply(image_vectors_normal[,3:ncol(image_vectors_normal)], 2, sd)
sd_effusion <- apply(image_vectors_effusion[,3:ncol(image_vectors_effusion)], 2, sd)
```

```{r max histogramas media, include=FALSE}
# Para calcular los rangos de las frecuencias
hmn <- hist(media_normal)
hme <- hist(media_effusion)
max_y <- ceiling(max(c(hmn$counts, hme$counts))/100)*100
```

```{r histogramas media}
par(mfrow= c(1,2))
hist(media_normal,
     main="Valor medio de las variables\nClase normal",
     ylab = "Frecuencia",
     xlab = "Media",
     xlim = c(0,1),
     ylim = c(0, max_y))

hist(media_effusion,
     main="Valor medio de las variables\nClase effusion",
     ylab = "Frecuencia",
     xlab = "Media",
     xlim = c(0,1),
     ylim = c(0, max_y))
```

Los valores medios de las variables en la clase "normal" parecen estar más concentrados en el centro de la distribución, mientras que en la clase "effusion" la distribución es más achatada. Esto podría significar que las imágenes de la clase "effusion" presentan áreas más claras y más oscuras que las imágenes de la clase "normal".

## Histogramas de las desviaciones típicas
```{r max histogramas desviación típica, include=FALSE}
# Para calcular los rangos de las frecuencias
hsn <- hist(sd_normal)
hse <- hist(sd_effusion)
max_ye <- ceiling(max(c(hsn$counts, hse$counts))/100)*100
max_xe <- max(c(sd_effusion, sd_normal))
```

```{r histogramas desviacion tipica}
par(mfrow= c(1,2))
hist(sd_normal,
     main="Desviación típica de las variables\nClase normal",
     ylab = "Frecuencia",
     xlab = "Media",
     xlim = c(0,max_xe),
     ylim = c(0, max_ye))

hist(sd_effusion,
     main="Desviación típica de las variables\nClase effusion",
     ylab = "Frecuencia",
     xlab = "Desviación típica",
     xlim = c(0,max_xe),
     ylim = c(0, max_ye))
```

Por la forma de los histogramas, y los rangos de valores en los que se mueven, parece que las variales en las observaciones de clase "effusion" presentan valores de desviación típica mayores que las variables en la clase "normal".


## Contraste de valores medios (t de Student)

Utilizando el test de la t de Student, hemos comparado los descriptores de ambos grupos de observaciones. Una vez ajustados los p-valores según el método BH, para tener en cuenta que estamos haciendo comparaciones múltiples, podemos ver en la siguiente tabla los 25 descriptores estadísticamente más significativos y la diferencia entre las medias de ambos grupos para cada descriptor.
```{r test t}
#Check if the file already exists
if(file.exists("./results/variables_t_test.RData")){
  load("./results/variables_t_test.RData")
}else{
# Contraste de valores medios (t-test)
# Código adaptado de
# https://stackoverflow.com/questions/13790611/apply-t-test-on-many-columns-in-a-dataframe-split-by-factor
variables_t_test <- t(sapply(image_vectors[c(-1,-2)], function(x)
  unlist(t.test(x~image_vectors$cat)[c("estimate", "p.value")])))
save(variables_t_test, file = "./results/variables_t_test.RData")
}
# This if-else block is to avoid re-calculating 
# the object every time I want to try the code.
# I have just have care to remove the .RData file
# if I want to re-calculate it.

## TODO Refactoriza todos los casos en los que NO realizo un cálculo si determinado fichero ya existe => Convertir en función
```

```{r ajustar p-valores por correccion de Benjamini & Hochberg (BH), results='asis'}
variables_t_test_adjusted <- as.data.frame(variables_t_test)
variables_t_test_adjusted[,3] <- p.adjust(variables_t_test_adjusted[,3])
# Ordenar los valores de menor a mayor
ordered_p_values <- variables_t_test_adjusted[order(variables_t_test[,3]),]
# Añadir columna con diferencia de medias
ordered_p_values[, "diferencia"] <- ordered_p_values[,1] - ordered_p_values[,2]
ordered_p_values$p.value <- format(ordered_p_values$p.value, digits = 4)
# Preparar para mostrar como tabla
kable(ordered_p_values[1:25,-c(1,2)],
      caption = "Tabla con los 25 descriptores de menor p-valor",
      digits = 4,
      col.names = c("p-valores", "Diferencia"),
      scientific=TRUE)


#library(xtable)
#adjusted_table <- xtable(ordered_p_values[1:25,-c(1,2)],
#       caption = "Tabla con los 25 descriptores de menor p-valor",
#       digits = 4,
#       display=c("s", "e", "fg"))
#print(adjusted_table, type="latex", comment = FALSE)
```

Me llama la atención que en estos descriptores estadísticamente más significativos, la diferencia entre las medias del grupo "derrame"" y del grupo "normal" es de alrededor del 20%, que no parece especialmente grande.

```{r}
## TODO: Normalizar los p-valores ajustados, asociarlos a una escala de color y reconstruir la imagen para saber qué zonas de la imagen son más informativas.
```

# Exploración con el algoritmo k-NN

```{r separa sets entrenamiento y prueba}
train <- 0.67
test <- 1-train
set.seed(123)
# Reordered row numbers
observaciones <- nrow(image_vectors)
shuffled_rows <- sample(observaciones)
training_rows <- shuffled_rows[1:(observaciones*train)]
test_rows <- shuffled_rows[((observaciones*train)+1):observaciones]
# Remember to use only numeric variables
image_vectors_training <- image_vectors[training_rows, 3:ncol(image_vectors)]
image_vectors_test <- image_vectors[test_rows, 3:ncol(image_vectors)]

# Category labels
image_vectors_train_labels <- image_vectors[training_rows, 1]
image_vectors_test_labels <- image_vectors[test_rows, 1]

## TODO Para el número de filas que se usan como set de entrenamiento,
# hay que tener en cuenta un redondeo para las ocasiones en que no haya
# múltiplos exactos.
```

Antes de aplicar el algoritmo hemos dividido el set de datos, al azar, en un set de entrenamiento (`r noquote(paste0(train*100,"%"))` de las observaciones) y un set de prueba (`r noquote(paste0(test*100,"%"))` de los datos).

El set de entrenamiento (`r observaciones*train` observaciones en este caso) es la referencia que usa el algoritmo para realizar la clasifición de las observaciones.

El set de prueba (`r observaciones*test` observaciones en este caso) sirve para evaluar la capacidad del algoritmo para clasificar correctamente cada observación dentro del grupo que le corresponde.

# Evaluación de diferentes valores de _k_
```{r classify the test data and evaluate different k values}

k <- c(3, 5, 7, 11, 23, 45, 67)

evaluator <- function (x){
  image_vectors_test_pred <- knn(train = image_vectors_training,
                               test = image_vectors_test,
                               cl = image_vectors_train_labels, 
                               k=x)
  Results_table <- prop.table(
    table(image_vectors_test_labels,
          image_vectors_test_pred))
  return(c(x, Results_table[3], Results_table[2]))
}

#Check if the file already exists
if(file.exists("./results/evaluated_k.RData")){
  load("./results/evaluated_k.RData")
}else{
# Evaluate the different k values
  evaluated_k <- sapply(k, evaluator)
  save(evaluated_k, file="./results/evaluated_k.RData")
}
# This if-else block is to avoid re-calculating 
# the object every time I want to try the code.
# I have just have care to remove the .RData file
# if I want to re-calculate it.


# Format results as a data frame
dtevaluated_k <- as.data.frame(t(evaluated_k))
# Add column for total errors of classifying
dtevaluated_k[,4] <- (dtevaluated_k[,2] + dtevaluated_k[,3])
# Add column names
colnames(dtevaluated_k) <- c("k","Falsos Negativos", "Falsos Positivos", "Error Total")
# Format as percentages
dtevaluated_k[,2:4] <- round(dtevaluated_k[,2:4], 3)*100
dtevaluated_k[,2:4] <- mapply(paste0, dtevaluated_k[,2:4], "%")

# Print table
knitr::kable(dtevaluated_k,
             align = c("c","c","c","c"))
```

Si priorizamos los falsos negativos, que son lo que potencialmente suponen un mayor perjuicio para el paciente, el k más adecuado sería `r evaluated_k[1, (which.min(evaluated_k[2,]))]`; que nos proporciona un valor del `r noquote(paste0(round(evaluated_k[2, (which.min(evaluated_k[2,]))], digits=3)*100, "%"))` de falsos positivos.

## Análisis de rendimiento. Curvas ROC y AUC

Las curvas ROC (Receiver Operating Characteristic) son un método de análisis de rendimiento que se usa para determinar el punto de equilibrio entre la capacidad de detectar positivos auténticos, y la de evitar falsos positivos.

Un clasificador perfecto detectaría todos los positivos antes de detectar ningún falso negativo. Una forma de medir cuánto se acerca el rendimiento de nuestro clasificador al del clasificador perfecto es mediante el estadístico AUC (area under the ROC curve).

En los siguientes gráficos están representadas tanto las curvas ROC como los valores AUC para cada valor de k:

```{r create ROC curve}

# Necesitaremos los valores reales:
# image_vectors_test_labels
# Y los valores predichos:
# image_vectors_test_pred (producidos dentro de la función evaluator())
prob_pred_ <- function(x){
image_vectors_test_pred <- knn(train = image_vectors_training,
                               test = image_vectors_test,
                               cl = image_vectors_train_labels, 
                               k=x,
                               prob = TRUE)

# Probabilidad de la clase predicha
prob_pred <- attr(image_vectors_test_pred, "prob")


# Probabilidad de la clase positiva ("effusion")
for (i in 1:length(image_vectors_test_pred)){
    if(image_vectors_test_pred[i]=="n"){
      prob_pred[i]<- 1-prob_pred[i]
    }
}
return(prob_pred)
}
## TODO: Refactorizar la funcion evaluator para no calcular image_vectors_test_pred dos veces.

#Check if the file already exists
if(file.exists("./results/prob_pred.RData")){
  load("./results/prob_pred.RData")
}else{
# Calculate de probabilities of predicted positive class
  prob_pred <- sapply(k, prob_pred_)
  save(prob_pred, file="./results/prob_pred.RData")
}

# Create the prediction object
#prediction_object <- prediction(predictions = prob_pred,
#                                labels = image_vectors_test_labels,
#                                label.ordering = c("n", "e"))

prediction_objects <- apply(prob_pred, 2, prediction,
                            labels = image_vectors_test_labels,
                            label.ordering = c("n", "e"))
# Create performance object
#performance_object <- performance(prediction_object,
#                                  measure = "tpr",
#                                  x.measure = "fpr")

performance_objects <- lapply(prediction_objects,
                              performance,
                              measure = "tpr",
                              x.measure = "fpr")

# Calculate AUC
#perf.auc <- performance(prediction_object, measure = "auc")
perf.aucs <- lapply(prediction_objects,
                    performance,
                    measure="auc")

#auc_value <- unlist(perf.auc@y.values)
auc_values <- lapply(perf.aucs, function (x) unlist(x@y.values))
```

```{r draw ROC curve, fig.height = 20, fig.width = 8}
# Draw ROC curve
par(mfrow=c(4,2))
for(i in 1:length(performance_objects)){
plot(performance_objects[[i]],
     main=paste0("Curva ROC para k = ",k[i]),
     col = "blue", lwd=3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
text(0.1, y = 0.9, labels = paste0("AUC: ", round(auc_values[[i]], digits = 2)))
}
```


# Comentario

Una vez calculadas las curvas ROC y los valores de AUC, podemos ver que el mayor AUC (`r round(unlist((auc_values[which.max(auc_values)])), digits = 2)`) corresponde al valor de k = `r k[which.max(auc_values)]`.

El valor k para el mínimo de falsos negativos (`r noquote(paste0(round(evaluated_k[2, (which.min(evaluated_k[2,]))], digits=3)*100, "%"))`) corresponde a k = `r evaluated_k[1, (which.min(evaluated_k[2,]))]`.

El valor k para el mínimo de falsos positivos (`r noquote(paste0(round(evaluated_k[3, (which.min(evaluated_k[3,]))], digits=3)*100, "%"))`) corresponde a k = `r evaluated_k[1, (which.min(evaluated_k[3,]))]`.

El valor k para el error total mínimo (`r noquote(paste0(round(min(apply(evaluated_k[2:3,], 2, sum)), digits=3)*100, "%"))`) es `r evaluated_k[1, (which.min(apply(evaluated_k[2:3,], 2, sum)))]`.

Dado que el error potencialmente más perjudicial para el paciente son los falsos negativos, deberíamos elegir el valor de k = `r evaluated_k[1, (which.min(evaluated_k[2,]))]`. Este resulta en un falso negativo del `r noquote(paste0(round(evaluated_k[2, (which.min(evaluated_k[2,]))], digits=3)*100, "%"))`, falso positivo del `r noquote(paste0(round(evaluated_k[3, (which.min(evaluated_k[2,]))], digits=3)*100, "%"))` y AUC de `r round(unlist((auc_values[which.min(evaluated_k[2,])])), digits = 2)`.


\newpage
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
sessionInfo() # For better reproducibility
```








