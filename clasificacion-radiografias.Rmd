---
title: "Clasificación de imagenes de radiografias de tórax entre normales y con derrame"
subtitle: "Machine Learning - PEC 1"
author: "Jorge Vallejo Ortega"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document: 
      toc: true
      toc_float: true
  pdf_document:
      toc: true
# Next code for knitting both types of documents automatically comes from https://stackoverflow.com/questions/39662365/knit-one-markdown-file-to-two-output-files/53280491#53280491
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_format = "all") })
bibliography: scholar.bib
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE)

```

```{r libraries, include=FALSE}
# Install packages
# Load packages
# ...

library(knitr)
```

# Índice

# Algoritmo k-NN

El algoritmo de los k vecinos más próximos (_k-nearest neighbours_) es un algoritmo de aprendizaje automático (_machine learning_) que se utiliza para clasificar observaciones, según si sus características las hacen más parecidas a uno u otro grupo ya establecidos.  

En una primera fase de 'entrenamiento', una colección de observaciones ya clasificadas se distribuyen en un espacio _n_-dimensional. Cada dimensión corresponde a una de las variables medidas en las observaciones. Las nuevas observaciones, las cuales queremos clasificar, se distribuyen a su vez en ese espacio _n_-dimensional; y se clasifican dentro de los grupos a los que pertenezcan aquellas otras observaciones, ya clasificadas, de las que más cerca se encuentren. El número de observaciones conocidas que tenemos en cuenta para clasificar las observaciones nuevas, es ése número _k_.


## Fortalezas y debilidades del algoritmo

```{r fortalezas debilidades, echo=FALSE, results='asis'}
fort_deb <- matrix(c("*Simple y efectivo",
              "* No hace asunciones acerca de la distribución subyacente de los datos",
              "* Fase de entrenamiento rápida",
              " ",
              "* No produce un modelo, limitando la habilidad para entender cómo las características se relacionan con la clasificación",
              "* Requiere seleccionar una _k_ adecuada",
              "* Fase de clasificación lenta",
              "* Se requiere procesado adicional para características nominales y datos incompletos"),
              ncol=2)
col_names <- c("Fortalezas", "Debilidades")
kable(fort_deb, col.names = col_names)
```


# Pre-procesado de datos

Las imágenes a clasificar deberán estar en una carpeta llamada "dataset", en el mismo directorio que el código que vamos a correr.
```{r set directories structure}
dir.create("./results")
```

```{r function image to vector}
library('OpenImageR')

image_tovector <- function(x){
# Create filepath
filepath <- paste("./dataset", sep='/',
                  paste(as.vector(x), collapse = '/'))
#print(filepath)
# Read image
image <- readImage(filepath)
# Convert to greyscale
image_grey <- rgb_2gray(image)
# Resize to 64x64
image_resized <- resizeImage(image_grey, 64, 64)
# Transform from matrix to vector
return(as.vector(image_resized))
}

## TODO ## Incluir en el informe ejemplos de imagenes antes y después del preprocesado (función imageShow()).

```

```{r extract directories and filenames}
directories <- list.dirs("./dataset", full.names = FALSE, recursive = FALSE)
filenames <- as.data.frame(
  sapply(directories,
                    function (x){return(
                      list.files(paste0("./dataset/", x)))}))

# From wide to long format
filenames <- reshape(filenames,
                      direction="long",
                      varying = c("effusion", "normal"),
                      v.names="Filename",
                      timevar="cat",
                      times = c("effusion", "normal")
                      )
row.names(filenames) <- NULL
filenames$id <- NULL
#filenames$Condition <- as.factor(filenames$Condition)
filenames$Filename <- as.character(filenames$Filename)
```

```{r create dataframe of vectorized images}
# Matrix of vectorized images
image_vectors <- apply(filenames, 1, image_tovector)
## TODO Keep the file names as col names

# This instructions to avoid re-calculating the object every time
load("image_vectors.RData")
save(image_vectors, file = "image_vectors.RData")

# Transpose to have images as observations
image_vectors <- as.data.frame(t(image_vectors))
```

```{r test images in rows, eval=FALSE}
## How to check if each row if really an x-ray image:
test_row <- function(x){
# Extract one row of the dataframe and store as a numeric vector
 test <- as.numeric(image_vectors[x,])
# Transform vector into matrix
 test <- matrix(test, ncol=64)
# Check if you obtain an x-ray image
 imageShow(test)
}
```

```{r final dataframe with filenames and diagnostic}
image_vectors <- cbind(filenames, image_vectors)
# Change cat variable to factors
image_vectors$cat <- as.factor(image_vectors$cat)
levels(image_vectors$cat) <- c("e", "n")
```

```{r write the dataframe into a csv file}
write.csv2(image_vectors,
           file="./results/RX_Torax_4097.csv",
           row.names = FALSE)
```

### Lectura de datos
```{r lectura del fichero csv}
image_vectors <- read.csv2("./results/RX_Torax_4097.csv")
```


### Estructura de los datos
```{r examine data structure}
observaciones <- nrow(image_vectors)
variables <- ncol(image_vectors)-2
 
```

El set de datos examinado está compuesto por `r observaciones` observaciones, de las cuales se han tomado `r variables` variables, y están divididas en `r length(directories)` clases (`r directories`) that are codified as `r levels(image_vectors$cat)`.

La distribución de cada clase es la siguiente:

```{r}
kable(table(image_vectors$cat),
      col.names = c("Clase", "Frecuencia"),
      align = c('r','l'))
```

### Histograma de medias

```{r calcula la media de cada variable}
image_vectors_normal <- image_vectors[image_vectors$cat == 'n',]
image_vectors_effusion <- image_vectors[image_vectors$cat == 'e',]

media_normal <- apply(image_vectors_normal[,3:ncol(image_vectors_normal)], 2, mean)
media_effusion <- apply(image_vectors_effusion[,3:ncol(image_vectors_effusion)], 2, mean)
```


```{r calcula la desviación típica de cada variable}
sd_normal <- apply(image_vectors_normal[,3:ncol(image_vectors_normal)], 2, sd)
sd_effusion <- apply(image_vectors_effusion[,3:ncol(image_vectors_effusion)], 2, sd)
```

```{r max histogramas media}
# Para calcular los rangos de las frecuencias
hmn <- hist(media_normal)
hme <- hist(media_effusion)
max_y <- ceiling(max(c(hmn$counts, hme$counts))/100)*100
```
```{r histogramas media}
par(mfrow= c(1,2))
hist(media_normal,
     main="Valor medio de las variables\nClase normal",
     ylab = "Frecuencia",
     xlab = "Media",
     xlim = c(0,1),
     ylim = c(0, max_y))

hist(media_effusion,
     main="Valor medio de las variables\nClase effusion",
     ylab = "Frecuencia",
     xlab = "Media",
     xlim = c(0,1),
     ylim = c(0, max_y))
```

Los valores medios de las variables en la clase "normal" parecen estar más concentrados en el centro de la distribución, mientras que en la clase "effusion" la distribución es más achatada. Esto podría significar que las imágenes de la clase "effusion" presentan áreas más claras y más oscuras que las imágnes de la clase "normal".

```{r max histogramas desviación típica}
# Para calcular los rangos de las frecuencias
hsn <- hist(sd_normal)
hse <- hist(sd_effusion)
max_ye <- ceiling(max(c(hsn$counts, hse$counts))/100)*100
max_xe <- max(c(sd_effusion, sd_normal))
```
```{r histogramas desviación típica}
par(mfrow= c(1,2))
hist(sd_normal,
     main="Desviación típica de las variables\nClase normal",
     ylab = "Frecuencia",
     xlab = "Media",
     xlim = c(0,max_xe),
     ylim = c(0, max_ye))

hist(sd_effusion,
     main="Desviación típica de las variables\nClase effusion",
     ylab = "Frecuencia",
     xlab = "Desviación típica",
     xlim = c(0,max_xe),
     ylim = c(0, max_ye))
```

Por la forma de los histogramas, y los rangos de valores en los que se mueven, parece que las variales en las observaciones de clase "effusion" presentan valores de desviación típica mayores que las variables en la clase "normal".


## Contraste de valores medios (t-test)

```{r test t}
# Código adaptado de
# https://stackoverflow.com/questions/13790611/apply-t-test-on-many-columns-in-a-dataframe-split-by-factor
variables_t_test <- t(sapply(image_vectors[c(-1,-2)], function(x)
  unlist(t.test(x~image_vectors$cat)[c("estimate", "p.value")])))
```

```{r ajustar p-valores por correccion de Benjamini & Hochberg (BH)}
variables_t_test_adjusted <- as.data.frame(variables_t_test)
variables_t_test_adjusted[,3] <- p.adjust(variables_t_test_adjusted[,3])
# Ordenar los valores de menor a mayor
ordered_p_values <- variables_t_test_adjusted[order(variables_t_test[,3]),]
# Añadir columna con diferencia de medias
ordered_p_values[, "diferencia"] <- ordered_p_values[,1] - ordered_p_values[,2]
# Preparar para mostrar como tabla
library(xtable)
xtable(ordered_p_values[1:25,-c(1,2)],
       caption = "Tabla con los 25 descriptores de menor p-valor",
       digits = 4,
       display=c("s", "e", "fg"))
```

Me llama la atención que en estos descriptores estadísticamente más significativos, la diferencia entre las medias del grupo "derrame"" y del grupo "normal" es de alrededor del 20%.


## TODO: Normalizar los p-valores ajustados, asociarlos a una escala de color y reconstruir la imagen para saber qué zonas de la imagen son más informativas.






## Preparación de los datos - crear los dataset de entrenamiento y de test
```{r}
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
```

```{r storing the dignosis labels}
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
```

### Step 3 - training a model on the data

For the k-NN algorithm, the training phase involves no model building; the process of training a lazy learner involves storing the input data in a structured format.

```{r classify the test data}
library('class')

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
                      cl = wbcd_train_labels, k=21)
```

### Step 4 - evaluating model performance

